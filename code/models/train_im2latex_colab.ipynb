{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Im2LaTeX Training - Complete Notebook\n",
        "\n",
        "This notebook combines all training code for mathematical formula recognition using CNN + LSTM architecture.\n",
        "\n",
        "Optimized for **Kaggle** and **Google Colab**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-environment"
      },
      "source": [
        "## 1. Setup Environment & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee348ce0-6075-4ac0-bda0-945b1b8b5e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in: Google Colab\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "\n",
        "# Check if we're in Colab or Kaggle\n",
        "IN_COLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__, '__IPYTHON__') else False\n",
        "IN_KAGGLE = 'KAGGLE_URL_BASE' in os.environ\n",
        "\n",
        "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-preparation"
      },
      "source": [
        "## 2. Data Preparation & Download"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple dataset download - CORRECT VERSION\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def unpack_zip(file_name, destination_folder):\n",
        "    \"\"\"Extract ZIP file to destination folder\"\"\"\n",
        "    print(f\"üì¶ Extracting {file_name} to {destination_folder}...\")\n",
        "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_folder)\n",
        "    os.remove(file_name)\n",
        "    print(\"‚úÖ ZIP extraction completed!\")\n",
        "\n",
        "def unpack_targz(file_name, destination_folder):\n",
        "    \"\"\"Extract .tar.gz file to destination folder\"\"\"\n",
        "    print(f\"üì¶ Extracting {file_name} to {destination_folder}...\")\n",
        "    with tarfile.open(file_name, 'r:gz') as tar_ref:\n",
        "        tar_ref.extractall(destination_folder)\n",
        "    os.remove(file_name)\n",
        "    print(\"‚úÖ TAR.GZ extraction completed!\")\n",
        "\n",
        "def download_dataset():\n",
        "    print(\"üì• Downloading Im2LaTeX dataset...\")\n",
        "\n",
        "    url = \"https://zenodo.org/api/records/56198/files-archive\"\n",
        "\n",
        "    file_name = 'im2latex.zip'\n",
        "    destination_folder = 'datasets/im2latex/'\n",
        "\n",
        "    # Download\n",
        "    response = requests.get(url)\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"‚úÖ Downloaded: {len(response.content) / (1024*1024):.1f} MB\")\n",
        "\n",
        "    # Ensure the destination folder exists\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    unpack_zip(file_name, destination_folder)\n",
        "\n",
        "    file_name = 'datasets/im2latex/formula_images.tar.gz'\n",
        "    destination_folder = 'datasets/im2latex/'\n",
        "\n",
        "    # Ensure the destination folder exists\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    unpack_targz(file_name, destination_folder)\n",
        "\n",
        "    # Show what we got\n",
        "    print(\"\\nüìÅ Files extracted:\")\n",
        "    for item in os.listdir(\"datasets/im2latex\"):\n",
        "        print(f\"   - {item}\")\n",
        "\n",
        "# Run it\n",
        "download_dataset()\n",
        "# Create directory\n",
        "os.makedirs('/content/models', exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsKqbgPoC5-r",
        "outputId": "f8b39053-3d76-4e52-b2cd-00a01159bac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading Im2LaTeX dataset...\n",
            "‚úÖ Downloaded: 292.6 MB\n",
            "üì¶ Extracting im2latex.zip to datasets/im2latex/...\n",
            "‚úÖ ZIP extraction completed!\n",
            "üì¶ Extracting datasets/im2latex/formula_images.tar.gz to datasets/im2latex/...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2646824500.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar_ref.extractall(destination_folder)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ TAR.GZ extraction completed!\n",
            "\n",
            "üìÅ Files extracted:\n",
            "   - im2latex_validate.lst\n",
            "   - formula_images\n",
            "   - im2latex_train.lst\n",
            "   - im2latex_formulas.lst\n",
            "   - readme.txt\n",
            "   - im2latex_test.lst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import sys\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "ROOT = Path(\"/content\")\n",
        "DATASET_ROOT = ROOT / \"datasets\" / \"im2latex\"\n",
        "OUTPUT_PATH = ROOT / \"datasets\" / \"im2latex_prepared_tokens.json\"\n",
        "VOCAB_PATH = ROOT / \"models\" / \"vocab.json\"  # New vocab path\n",
        "\n",
        "FORMULA_FILE = DATASET_ROOT / \"im2latex_formulas.lst\"\n",
        "TRAIN_LST = DATASET_ROOT / \"im2latex_train.lst\"\n",
        "VAL_LST = DATASET_ROOT / \"im2latex_validate.lst\"\n",
        "TEST_LST = DATASET_ROOT / \"im2latex_test.lst\"\n",
        "IMG_ROOT = DATASET_ROOT / \"formula_images\"\n",
        "\n",
        "def tokenize_latex(latex_str):\n",
        "    \"\"\"Properly tokenize LaTeX into meaningful tokens.\"\"\"\n",
        "    # Remove comments first\n",
        "    latex_str = re.sub(r'%.*$', '', latex_str, flags=re.MULTILINE)\n",
        "\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    n = len(latex_str)\n",
        "\n",
        "    while i < n:\n",
        "        # 1. LaTeX commands\n",
        "        if latex_str[i] == '\\\\':\n",
        "            cmd_match = re.match(r'\\\\([a-zA-Z]+|[^a-zA-Z])', latex_str[i:])\n",
        "            if cmd_match:\n",
        "                cmd = '\\\\' + cmd_match.group(1)\n",
        "                tokens.append(cmd)\n",
        "                i += len(cmd)\n",
        "                continue\n",
        "\n",
        "        # 2. Match environments\n",
        "        if latex_str[i:i+6] == '\\\\begin':\n",
        "            env_match = re.match(r'\\\\begin\\{([^}]+)\\}', latex_str[i:])\n",
        "            if env_match:\n",
        "                tokens.append('\\\\begin{' + env_match.group(1) + '}')\n",
        "                i += len('\\\\begin{' + env_match.group(1) + '}')\n",
        "                continue\n",
        "\n",
        "        if latex_str[i:i+4] == '\\\\end':\n",
        "            env_match = re.match(r'\\\\end\\{([^}]+)\\}', latex_str[i:])\n",
        "            if env_match:\n",
        "                tokens.append('\\\\end{' + env_match.group(1) + '}')\n",
        "                i += len('\\\\end{' + env_match.group(1) + '}')\n",
        "                continue\n",
        "\n",
        "        # 3. Special characters\n",
        "        if latex_str[i] in '{}[]()^_$':\n",
        "            tokens.append(latex_str[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 4. Operators\n",
        "        if latex_str[i] in '+-*/=<>':\n",
        "            tokens.append(latex_str[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 5. Punctuation\n",
        "        if latex_str[i] in ',.;!?':\n",
        "            tokens.append(latex_str[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 6. Numbers\n",
        "        if latex_str[i].isdigit():\n",
        "            tokens.append(latex_str[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 7. Letters\n",
        "        if latex_str[i].isalpha():\n",
        "            tokens.append(latex_str[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 8. Whitespace (skip)\n",
        "        if latex_str[i].isspace():\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 9. Anything else\n",
        "        tokens.append(latex_str[i])\n",
        "        i += 1\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def build_vocab_from_formulas(formulas, min_freq=2):\n",
        "    \"\"\"Build vocabulary from all formulas.\"\"\"\n",
        "    all_tokens = []\n",
        "\n",
        "    for formula in formulas:\n",
        "        tokens = tokenize_latex(formula)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    token_counts = Counter(all_tokens)\n",
        "\n",
        "    vocab = {\n",
        "        '<SOS>': 0,\n",
        "        '<EOS>': 1,\n",
        "        '<PAD>': 2,\n",
        "        '<UNK>': 3,\n",
        "    }\n",
        "\n",
        "    idx = len(vocab)\n",
        "\n",
        "    # Add tokens that meet frequency threshold\n",
        "    for token, count in token_counts.most_common():\n",
        "        if count >= min_freq and token not in vocab:\n",
        "            vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "    print(f\"Built vocabulary with {len(vocab)} tokens\")\n",
        "\n",
        "    # Save vocabulary to /content/models/vocab.json\n",
        "    VOCAB_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"‚úÖ Saved vocabulary to {VOCAB_PATH}\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def load_formulas():\n",
        "    with open(FORMULA_FILE, \"r\", encoding=\"latin-1\") as f:\n",
        "        formulas = [line.strip() for line in f]\n",
        "    print(f\"Loaded {len(formulas)} formulas\")\n",
        "    return formulas\n",
        "\n",
        "def load_split(lst_path, formulas, vocab):\n",
        "    samples = []\n",
        "    with open(lst_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            f_id, img_name, _ = line.strip().split()\n",
        "            f_id = int(f_id)\n",
        "            img_path = IMG_ROOT / (img_name + \".png\")\n",
        "            if img_path.exists():\n",
        "                latex_formula = formulas[f_id]\n",
        "                tokens = tokenize_latex(latex_formula)\n",
        "                token_ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "                samples.append({\n",
        "                    \"img_path\": str(img_path),\n",
        "                    \"tokens\": token_ids,  # Store token IDs instead of string\n",
        "                    \"latex\": latex_formula  # Keep original for reference\n",
        "                })\n",
        "    print(f\"Loaded {len(samples)} samples from {lst_path.name}\")\n",
        "    return samples\n",
        "\n",
        "def main():\n",
        "    # Load formulas and build vocabulary\n",
        "    formulas = load_formulas()\n",
        "    vocab = build_vocab_from_formulas(formulas, min_freq=2)\n",
        "\n",
        "    # Load splits with tokenized data\n",
        "    dataset = {\n",
        "        \"train\": load_split(TRAIN_LST, formulas, vocab),\n",
        "        \"val\": load_split(VAL_LST, formulas, vocab),\n",
        "        \"test\": load_split(TEST_LST, formulas, vocab),\n",
        "        \"vocab\": vocab  # Include vocabulary in the dataset\n",
        "    }\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved tokenized dataset to: {OUTPUT_PATH}\")\n",
        "    print(f\"Dataset includes vocabulary with {len(vocab)} tokens\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpDHU1TNOZgv",
        "outputId": "5c2be4b9-fc9a-414f-bde5-34b3559f7331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 104564 formulas\n",
            "Built vocabulary with 549 tokens\n",
            "‚úÖ Saved vocabulary to /content/models/vocab.json\n",
            "Loaded 83884 samples from im2latex_train.lst\n",
            "Loaded 9320 samples from im2latex_validate.lst\n",
            "Loaded 10355 samples from im2latex_test.lst\n",
            "\n",
            "Saved tokenized dataset to: /content/datasets/im2latex_prepared_tokens.json\n",
            "Dataset includes vocabulary with 549 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 3. Preprocessing Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocessing-code"
      },
      "outputs": [],
      "source": [
        "def detect_content_bounding_box(img, background_threshold=240, padding=10):\n",
        "    \"\"\"Detect the bounding box of actual content (non-background) in the image.\"\"\"\n",
        "    if len(img.shape) == 3 and img.shape[2] == 4:  # RGBA\n",
        "        alpha = img[:, :, 3]\n",
        "        mask = alpha > 10  # Non-transparent areas\n",
        "    else:\n",
        "        mask = img < background_threshold\n",
        "\n",
        "    if not np.any(mask):\n",
        "        return 0, 0, img.shape[1], img.shape[0]\n",
        "\n",
        "    rows = np.any(mask, axis=1)\n",
        "    cols = np.any(mask, axis=0)\n",
        "\n",
        "    y1, y2 = np.where(rows)[0][[0, -1]]\n",
        "    x1, x2 = np.where(cols)[0][[0, -1]]\n",
        "\n",
        "    y1 = max(0, y1 - padding)\n",
        "    y2 = min(img.shape[0], y2 + padding)\n",
        "    x1 = max(0, x1 - padding)\n",
        "    x2 = min(img.shape[1], x2 + padding)\n",
        "\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def smart_crop_and_resize(img_gray, target_h=128, target_w=512, min_aspect_ratio=0.2, max_aspect_ratio=5.0):\n",
        "    \"\"\"Smart cropping and resizing that preserves content readability.\"\"\"\n",
        "    x1, y1, x2, y2 = detect_content_bounding_box(img_gray)\n",
        "    cropped = img_gray[y1:y2, x1:x2]\n",
        "\n",
        "    if cropped.size == 0:\n",
        "        cropped = img_gray\n",
        "\n",
        "    h, w = cropped.shape\n",
        "    current_aspect = w / h\n",
        "    constrained_aspect = max(min_aspect_ratio, min(max_aspect_ratio, current_aspect))\n",
        "\n",
        "    if constrained_aspect > (target_w / target_h):\n",
        "        new_w = target_w\n",
        "        new_h = int(target_w / constrained_aspect)\n",
        "    else:\n",
        "        new_h = target_h\n",
        "        new_w = int(target_h * constrained_aspect)\n",
        "\n",
        "    new_w = max(new_w, 32)\n",
        "    new_h = max(new_h, 32)\n",
        "\n",
        "    resized = cv2.resize(cropped, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
        "    padded = np.ones((target_h, target_w), dtype=np.uint8) * 255\n",
        "\n",
        "    y_offset = (target_h - new_h) // 2\n",
        "    x_offset = (target_w - new_w) // 2\n",
        "\n",
        "    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
        "\n",
        "    return padded\n",
        "\n",
        "def adaptive_binarize(img_gray, method='otsu', block_size=35, C=10):\n",
        "    \"\"\"Adaptive binarization for mathematical formulas.\"\"\"\n",
        "    if method == 'otsu':\n",
        "        _, binary = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    else:\n",
        "        binary = cv2.adaptiveThreshold(\n",
        "            img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY, block_size, C\n",
        "        )\n",
        "    return binary\n",
        "\n",
        "def preprocess_for_model(image_path: str, target_h=128, target_w=512):\n",
        "    \"\"\"Improved preprocessing for mathematical formula images.\"\"\"\n",
        "    img = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load image: {image_path}\")\n",
        "\n",
        "    if len(img.shape) == 3:\n",
        "        if img.shape[2] == 4:  # RGBA\n",
        "            rgb = img[:, :, :3]\n",
        "            alpha = img[:, :, 3]\n",
        "            img_gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
        "            img_gray[alpha < 10] = 255\n",
        "        else:  # RGB\n",
        "            img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        img_gray = img\n",
        "\n",
        "    processed = smart_crop_and_resize(img_gray, target_h=target_h, target_w=target_w)\n",
        "    binary_adaptive = adaptive_binarize(processed, method='adaptive')\n",
        "    binary_otsu = adaptive_binarize(processed, method='otsu')\n",
        "\n",
        "    adaptive_non_white = np.sum(binary_adaptive < 128)\n",
        "    otsu_non_white = np.sum(binary_otsu < 128)\n",
        "\n",
        "    if adaptive_non_white > otsu_non_white * 0.3:\n",
        "        binary = binary_adaptive\n",
        "    else:\n",
        "        binary = binary_otsu\n",
        "\n",
        "    kernel = np.ones((2, 2), np.uint8)\n",
        "    cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
        "    normalized = (255 - cleaned) / 255.0\n",
        "\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-architecture"
      },
      "source": [
        "## 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-code"
      },
      "outputs": [],
      "source": [
        "class Im2Latex(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2, dropout=0.3):\n",
        "        super(Im2Latex, self).__init__()\n",
        "\n",
        "        # CNN Encoder for 128x512 images\n",
        "        self.cnn_encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # [64, 64, 256]\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # [128, 32, 128]\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # [256, 16, 64]\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # [512, 8, 32]\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # [512, 4, 32]\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((4, 32))  # [512, 4, 32]\n",
        "        )\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=2)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=512 * 4 + embed_dim,  # CNN features + embeddings\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, images, input_tokens):\n",
        "        # CNN features\n",
        "        cnn_features = self.cnn_encoder(images)  # [B, 512, 4, 32]\n",
        "        batch_size = cnn_features.size(0)\n",
        "        cnn_features = cnn_features.view(batch_size, 512 * 4, -1)  # [B, 2048, 32]\n",
        "        cnn_features = cnn_features.permute(0, 2, 1)  # [B, 32, 2048]\n",
        "\n",
        "        # Embed tokens\n",
        "        token_embeddings = self.embedding(input_tokens)  # [B, seq_len, embed_dim]\n",
        "\n",
        "        # Repeat CNN features for each time step\n",
        "        seq_len = input_tokens.size(1)\n",
        "        cnn_features = cnn_features.repeat(1, seq_len // 32 + 1, 1)[:, :seq_len, :]\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([cnn_features, token_embeddings], dim=-1)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.output_proj(lstm_out)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset-dataloader"
      },
      "source": [
        "## 5. Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-code"
      },
      "outputs": [],
      "source": [
        "class Im2LatexDataset(Dataset):\n",
        "    \"\"\"Dataset with preloading for faster training.\"\"\"\n",
        "\n",
        "    def __init__(self, data_list, vocab, max_len=256, cache_images=True, preload_images=True):\n",
        "        self.data = data_list\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        self.sos_token = vocab.get('<SOS>', 0)\n",
        "        self.eos_token = vocab.get('<EOS>', 1)\n",
        "        self.pad_token = vocab.get('<PAD>', 2)\n",
        "        self.unk_token = vocab.get('<UNK>', 3)\n",
        "\n",
        "        self.cache_images = cache_images\n",
        "        self.preload_images = preload_images\n",
        "        self._image_cache = {}\n",
        "\n",
        "        # Preload all images if requested\n",
        "        if self.preload_images:\n",
        "            print(f\"üîÑ Preloading {len(data_list)} images...\")\n",
        "            self._preload_all_images()\n",
        "        else:\n",
        "            print(f\"üìÅ Dataset created with {len(data_list)} samples (on-demand loading)\")\n",
        "\n",
        "    def _preload_all_images(self):\n",
        "        \"\"\"Preload all images into memory for faster training.\"\"\"\n",
        "        from concurrent.futures import ThreadPoolExecutor\n",
        "        import threading\n",
        "\n",
        "        loaded_count = 0\n",
        "        lock = threading.Lock()\n",
        "\n",
        "        def load_single_image(idx):\n",
        "            nonlocal loaded_count\n",
        "            item = self.data[idx]\n",
        "            img_path = item['img_path']\n",
        "\n",
        "            # Get full image path\n",
        "            if not img_path.startswith('/content/'):\n",
        "                full_path = f\"/content/datasets/im2latex/{img_path}\"\n",
        "            else:\n",
        "                full_path = img_path\n",
        "\n",
        "            try:\n",
        "                img = preprocess_for_model(full_path)\n",
        "                img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "                with lock:\n",
        "                    self._image_cache[img_path] = img_tensor\n",
        "                    loaded_count += 1\n",
        "\n",
        "                    if loaded_count % 1000 == 0:\n",
        "                        print(f\"  Loaded {loaded_count}/{len(self.data)} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {full_path}: {e}\")\n",
        "                with lock:\n",
        "                    self._image_cache[img_path] = torch.zeros(1, 128, 512, dtype=torch.float32)\n",
        "                    loaded_count += 1\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel loading\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            list(tqdm(executor.map(load_single_image, range(len(self.data))),\n",
        "                     total=len(self.data), desc=\"Preloading images\"))\n",
        "\n",
        "        print(f\"‚úÖ Preloaded {loaded_count} images into memory\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img_path = item['img_path']\n",
        "\n",
        "        # Use pre-tokenized token IDs\n",
        "        token_ids = item['tokens']\n",
        "\n",
        "        # Get image tensor from cache or load on-demand\n",
        "        if img_path in self._image_cache:\n",
        "            img_tensor = self._image_cache[img_path]\n",
        "        else:\n",
        "            # Get full image path\n",
        "            if not img_path.startswith('/content/'):\n",
        "                full_path = f\"/content/datasets/im2latex/{img_path}\"\n",
        "            else:\n",
        "                full_path = img_path\n",
        "\n",
        "            try:\n",
        "                img = preprocess_for_model(full_path)\n",
        "                img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "                # Cache if enabled\n",
        "                if self.cache_images:\n",
        "                    self._image_cache[img_path] = img_tensor\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {full_path}: {e}\")\n",
        "                img_tensor = torch.zeros(1, 128, 512, dtype=torch.float32)\n",
        "\n",
        "        # Build token sequence with SOS and EOS\n",
        "        tokens = [self.sos_token] + token_ids + [self.eos_token]\n",
        "\n",
        "        # Pad or truncate to max_len\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "            tokens[-1] = self.eos_token\n",
        "        else:\n",
        "            tokens.extend([self.pad_token] * (self.max_len - len(tokens)))\n",
        "\n",
        "        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "        # Create target sequence (shifted by one for teacher forcing)\n",
        "        target = tokens_tensor[1:].clone()\n",
        "        input_seq = tokens_tensor[:-1]\n",
        "\n",
        "        return img_tensor, input_seq, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    imgs, input_seqs, targets = zip(*batch)\n",
        "    imgs = torch.stack(imgs)\n",
        "    input_seqs = torch.stack(input_seqs)\n",
        "    targets = torch.stack(targets)\n",
        "    return imgs, input_seqs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-functions"
      },
      "source": [
        "## 6. Training & Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-code"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"One training epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    non_blocking = torch.cuda.is_available()\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", ncols=80)\n",
        "\n",
        "    for imgs, input_seqs, targets in pbar:\n",
        "        imgs = imgs.to(device, non_blocking=non_blocking)\n",
        "        input_seqs = input_seqs.to(device, non_blocking=non_blocking)\n",
        "        targets = targets.to(device, non_blocking=non_blocking)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs, input_seqs)\n",
        "\n",
        "        logits = logits.reshape(-1, logits.size(-1))\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        mask = (targets != 2)  # PAD token = 2\n",
        "        if mask.sum() > 0:\n",
        "            loss = criterion(logits[mask], targets[mask])\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        postfix = {'loss': f'{loss.item():.3f}', 'avg': f'{avg_loss:.3f}'}\n",
        "\n",
        "        if torch.cuda.is_available() and num_batches % 20 == 0:\n",
        "            gpu_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
        "            postfix['GPU'] = f'{gpu_memory:.1f}GB'\n",
        "\n",
        "        pbar.set_postfix(postfix)\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "    return avg_loss\n",
        "\n",
        "def validate(model, dataloader, criterion, device, vocab):\n",
        "    \"\"\"Proper validation with actual text generation and exact matching\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    exact_matches = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Reverse vocab for decoding\n",
        "    idx_to_char = {idx: char for char, idx in vocab.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, input_seqs, targets in tqdm(dataloader, desc=\"Validation\", ncols=100, leave=False):\n",
        "            imgs = imgs.to(device)\n",
        "            batch_size = imgs.size(0)\n",
        "\n",
        "            # REAL PREDICTION: Generate text from scratch for each image\n",
        "            batch_correct = 0\n",
        "            for i in range(batch_size):\n",
        "                # Get actual text from targets\n",
        "                actual_tokens = targets[i].cpu().numpy()\n",
        "                actual_text = ''.join([idx_to_char.get(tok, '?') for tok in actual_tokens\n",
        "                                      if tok not in [0, 1, 2]])  # Remove SOS/EOS/PAD\n",
        "\n",
        "                # Generate predicted text from image only\n",
        "                predicted_text = generate_from_image(model, imgs[i:i+1], vocab, device, max_len=256)\n",
        "\n",
        "                # Exact string comparison (since no whitespaces)\n",
        "                if predicted_text == actual_text:\n",
        "                    batch_correct += 1\n",
        "                else:\n",
        "                    # Print first few errors for debugging\n",
        "                    if total_samples < 3:\n",
        "                        print(f\"  ‚ùå Pred: '{predicted_text}'\")\n",
        "                        print(f\"  ‚úÖ Actual: '{actual_text}'\")\n",
        "                        print(\"  ---\")\n",
        "\n",
        "                total_samples += 1\n",
        "\n",
        "            exact_matches += batch_correct\n",
        "\n",
        "            # Keep original loss calculation for compatibility\n",
        "            input_seqs = input_seqs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(imgs, input_seqs)\n",
        "            logits = logits.reshape(-1, logits.size(-1))\n",
        "            targets = targets.reshape(-1)\n",
        "            mask = (targets != 2)  # PAD token\n",
        "            if mask.sum() > 0:\n",
        "                loss = criterion(logits[mask], targets[mask])\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "    accuracy = exact_matches / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    print(f\"  Validation Accuracy: {accuracy:.2%} ({exact_matches}/{total_samples} correct)\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def generate_from_image(model, image, vocab, device, max_len=256):\n",
        "    \"\"\"Generate LaTeX text from a single image (autoregressive)\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Reverse vocabulary for decoding\n",
        "    idx_to_char = {idx: char for char, idx in vocab.items()}\n",
        "    sos_token = vocab.get('<SOS>', 0)\n",
        "    eos_token = vocab.get('<EOS>', 1)\n",
        "\n",
        "    # Start with SOS token\n",
        "    current_token = torch.tensor([[sos_token]], device=device)\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_len):\n",
        "            # Get model prediction\n",
        "            logits = model(image, current_token)\n",
        "            next_token_logits = logits[0, -1, :]  # Get last token prediction\n",
        "\n",
        "            # Greedy decoding - take most likely token\n",
        "            next_token = torch.argmax(next_token_logits).item()\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token == eos_token:\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "            current_token = torch.cat([\n",
        "                current_token,\n",
        "                torch.tensor([[next_token]], device=device)\n",
        "            ], dim=1)\n",
        "\n",
        "    # Convert tokens to string\n",
        "    generated_text = ''.join([idx_to_char.get(tok, '?') for tok in generated_tokens])\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-training"
      },
      "source": [
        "## 7. Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-training-code"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Training parameters\n",
        "    json_path = \"/content/datasets/im2latex_prepared_tokens.json\"\n",
        "    batch_size = 32  # Reduced for stability with preloading\n",
        "    num_epochs = 30\n",
        "    learning_rate = 1e-4\n",
        "    max_len = 256\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading tokenized data from {json_path}...\")\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    train_data = data.get('train', [])\n",
        "    val_data = data.get('val', [])\n",
        "    vocab = data.get('vocab', {})\n",
        "\n",
        "    print(f\"Train samples: {len(train_data)}\")\n",
        "    print(f\"Val samples: {len(val_data)}\")\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Create datasets WITH PRELOADING\n",
        "    print(\"Creating datasets with preloading...\")\n",
        "    train_data_cropped = train_data[:20000]\n",
        "    val_data_cropped = val_data[:2000]\n",
        "    train_dataset = Im2LatexDataset(\n",
        "        train_data_cropped,\n",
        "        vocab,\n",
        "        max_len=max_len,\n",
        "        cache_images=True,\n",
        "        preload_images=True  # Enable preloading for training\n",
        "    )\n",
        "    val_dataset = Im2LatexDataset(\n",
        "        val_data_cropped,\n",
        "        vocab,\n",
        "        max_len=max_len,\n",
        "        cache_images=True,\n",
        "        preload_images=True  # Enable preloading for validation\n",
        "    ) if val_data_cropped else None\n",
        "\n",
        "    # DataLoader with optimized settings\n",
        "    num_workers = 4 if torch.cuda.is_available() else 0  # Increased workers for faster loading\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    ) if val_dataset else None\n",
        "\n",
        "    # Create model\n",
        "    print(\"Creating model...\")\n",
        "    model = Im2Latex(vocab_size=vocab_size).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=2)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\\nStarting training with preloaded data...\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # Training\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if val_loader:\n",
        "            val_loss, val_accuracy = validate(model, val_loader, criterion, device, vocab)\n",
        "            print(f\"Epoch {epoch}/{num_epochs} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.2%}\")\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_path = \"/content/models/im2latex_best.pth\"\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Saved best model to {best_model_path}\")\n",
        "        else:\n",
        "            scheduler.step(train_loss)\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            checkpoint_path = f\"/content/models/im2latex_epoch{epoch}.pth\"\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'vocab_size': vocab_size\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "        # Save vocabulary\n",
        "        vocab_path = \"/content/models/vocab.json\"\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Models saved to /content/models/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-training"
      },
      "source": [
        "## 8. Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-training-final",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab3c26d-08e3-40b1-9094-a85c939229fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CUDA device: Tesla T4\n",
            "CUDA memory: 14.74 GB\n",
            "Loading tokenized data from /content/datasets/im2latex_prepared_tokens.json...\n",
            "Train samples: 83884\n",
            "Val samples: 9320\n",
            "Vocabulary size: 549\n",
            "Creating datasets with preloading...\n",
            "üîÑ Preloading 20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:   5%|‚ñå         | 1002/20000 [01:00<16:56, 18.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 1000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  10%|‚ñà         | 2001/20000 [01:58<17:05, 17.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 2000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  15%|‚ñà‚ñå        | 3003/20000 [02:57<14:57, 18.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 3000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  20%|‚ñà‚ñà        | 4002/20000 [03:55<15:05, 17.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 4000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  25%|‚ñà‚ñà‚ñå       | 5000/20000 [04:53<15:00, 16.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 5000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  30%|‚ñà‚ñà‚ñà       | 6000/20000 [05:49<11:18, 20.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 6000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  35%|‚ñà‚ñà‚ñà‚ñå      | 7003/20000 [06:47<12:31, 17.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 7000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  40%|‚ñà‚ñà‚ñà‚ñà      | 8002/20000 [07:44<11:22, 17.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 8000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9000/20000 [08:41<15:01, 12.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 9000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10001/20000 [09:38<09:06, 18.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 10000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11001/20000 [10:37<12:40, 11.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 11000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 11997/20000 [11:33<07:54, 16.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 12000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13003/20000 [12:31<07:02, 16.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 13000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14002/20000 [13:28<04:50, 20.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 14000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15001/20000 [14:25<03:57, 21.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 15000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16002/20000 [15:23<03:39, 18.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 16000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 16998/20000 [16:20<02:45, 18.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 17000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17998/20000 [17:19<02:02, 16.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 18000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19002/20000 [18:16<00:50, 19.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 19000/20000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [19:14<00:00, 17.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 20000/20000 images...\n",
            "‚úÖ Preloaded 20000 images into memory\n",
            "üîÑ Preloading 2000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1002/2000 [00:56<00:51, 19.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 1000/2000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [01:54<00:00, 17.44it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 2000/2000 images...\n",
            "‚úÖ Preloaded 2000 images into memory\n",
            "Creating model...\n",
            "Model parameters: 12,204,325\n",
            "\n",
            "Starting training with preloaded data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                          | 0/625 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [03:03<00:00,  3.40it/s, loss=4.119, avg=4.223]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - Train Loss: 4.2234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidation:   0%|                                                            | 0/63 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚ùå Pred: '\\label{{{{{{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}'\n",
            "  ‚úÖ Actual: '\\int_{-\\epsilon}^\\inftydl\\:{\\rme}^{-l\\zeta}\\int_{-\\epsilon}^\\inftydl'{\\rme}^{-l'\\zeta}ll'{l'-l\\overl+l'}\\{3\\,\\delta''(l)-{3\\over4}t\\,\\delta(l)\\}=0.\\label{eq21}'\n",
            "  ---\n",
            "  ‚ùå Pred: '\\label{{{{{{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}'\n",
            "  ‚úÖ Actual: '\\label{hR=hR+hR+}\\hat{R}=d\\hat{C}-\\hat{C}\\wedge\\hat{H}_3=\\hat{R}_2\\oplus\\hat{R}_4\\oplus\\hat{R}_6\\oplus\\hat{R}_8\\oplus\\hat{R}_{10}\\;,'\n",
            "  ---\n",
            "  ‚ùå Pred: '\\label{{{{{{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}'\n",
            "  ‚úÖ Actual: '\\label{knsflowy}\\frac{\\partialy(l)}{\\partiall}=\\left[d-\\eta_y-2\\pi^2\\tilde{f}(d)K(l)\\right]~y(l),'\n",
            "  ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                         | 10/63 [11:32<1:01:06, 69.19s/it]"
          ]
        }
      ],
      "source": [
        "# Execute the training\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-results"
      },
      "source": [
        "## 9. Download Trained Models (Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-models"
      },
      "outputs": [],
      "source": [
        "# Download trained models from Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "\n",
        "    # Create zip of models\n",
        "    import shutil\n",
        "    shutil.make_archive('im2latex_models', 'zip', '/content/models')\n",
        "\n",
        "    # Download\n",
        "    files.download('im2latex_models.zip')\n",
        "    print(\"Models downloaded!\")\n",
        "else:\n",
        "    print(\"Models saved in /content/models/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}